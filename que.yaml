description: "TDS Virtual TA Project Sample (but not the actual evaluation) Questions"


providers:
  # 1) Your FastAPI /query endpoint
  - id: https
    config:
      url: "https://app.example.com/query"  # Replace this with your API endpoint
      method: POST
      headers:
        Content-Type: "application/json"
      body:
        question: "{{ question }}"
        image: "{{ image | default(null) }}"
      transformResponse: json

  # 2) Your OpenAI proxy for rubricâ€style evaluation
  - id: aiproxy
    config:
      url: "https://aiproxy.sanand.workers.dev/openai/v1/chat/completions"
      method: POST
      headers:
        Content-Type: "application/json"
        Authorization: "Bearer {{ env.OPENAI_API_KEY }}"
      body:
        model: "gpt-4o-mini"
        messages:
          - role: system
            content: >-
              You are an evaluator that checks if an output meets specific criteria.
              Analyze the output based on the given rubric and respond with a JSON
              object containing {"reason":"your analysis","score": number between
              0.0 and 1.0,"pass":true/false}.
          - role: user
            content: >-
              Output to evaluate: {{ output }}

              Rubric: {{ rubric }}
        temperature: 0
      transformResponse: json

# By default run all tests against your FastAPI endpoint via provider "https"
defaultTest:
  options:
    provider:
      id: https
tests:
  - vars:
      question: >-
        The question asks to use gpt-3.5-turbo-0125 model but the ai-proxy
        provided by Anand sir only supports gpt-4o-mini. So should we just use
        gpt-4o-mini or use the OpenAI API for gpt3.5 turbo?
      image: file://project-tds-virtual-ta-q1.webp
      link: >-
        https://discourse.onlinedegree.iitm.ac.in/t/ga5-question-8-clarification/155939
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Clarifies use of gpt-3.5-turbo-0125 not gpt-4o-mini
      - type: contains
        transform: JSON.stringify(output.links)
        value: >-
          https://discourse.onlinedegree.iitm.ac.in/t/ga5-question-8-clarification/155939
  - vars:
      question: >-
        If a student scores 10/10 on GA4 as well as a bonus, how would it appear
        on the dashboard?
      link: >-
        https://discourse.onlinedegree.iitm.ac.in/t/ga4-data-sourcing-discussion-thread-tds-jan-2025/165959/388
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Mentions the dashboard showing "110"
      - type: contains
        transform: JSON.stringify(output.links)
        value: >-
          https://discourse.onlinedegree.iitm.ac.in/t/ga4-data-sourcing-discussion-thread-tds-jan-2025/165959
  - vars:
      question: >-
        I know Docker but have not used Podman before. Should I use Docker for
        this course?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Recommends Podman for the course
      - type: llm-rubric
        transform: output.answer
        value: Mentions that Docker is acceptable
      - type: contains
        transform: JSON.stringify(output.links)
        value: https://tds.s-anand.net/#/docker
  - vars:
      question: When is the TDS Sep 2025 end-term exam?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Says it doesn't know (since this information is not available yet)


  - vars:
      question: >-
        Will there be partial marks for incorrect JSON responses in GA4?
      link: >-
        https://discourse.onlinedegree.iitm.ac.in/t/ga4-json-scoring-details/165984
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Clarifies partial marking criteria

  - vars:
      question: >-
        How do I access the Podman container logs for debugging errors?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Describes command or method for retrieving Podman logs

  - vars:
      question: >-
        What is the minimum score required in GA5 to pass the course?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Mentions the threshold clearly (e.g. 40%)

  - vars:
      question: >-
        Does the TDS course require GPU for local model testing?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Clarifies whether GPU is mandatory or optional



  - vars:
      question: >-
        What should I do if my proxy returns a 403 Forbidden error?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Suggests checking headers or authentication settings

  - vars:
      question: >-
        Is it okay to use third-party APIs in my evaluation module?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Mentions if external APIs are allowed or not

  - vars:
      question: >-
        Will the TDS end-term be open book or closed book?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Specifies the exam type (open/closed book)


  - vars:
      question: >-
        Do we need to include citations for all sources used in our GA submission?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: States whether citations are mandatory or optional

  - vars:
      question: >-
        How can I test my local setup to match the evaluation environment?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Recommends ways to simulate or match the evaluation container setup



  - vars:
      question: >-
        Is it okay to hard-code answers for specific inputs if I explain why?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Explains whether hardcoding is permitted and under what justification

  - vars:
      question: >-
        Will the evaluation script run with internet access or in a sandboxed environment?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Clearly mentions whether internet access is available during evaluation


writeLatestResults: true
commandLineOptions:
  cache: false